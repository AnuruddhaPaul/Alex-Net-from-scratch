{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaEKd1dJNBA1dfSCIbhBGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuruddhaPaul/Alex-Net-from-scratch/blob/main/Alex_Net_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8mJLnz2O_-B",
        "outputId": "e742c8b0-3951-4f0f-e83c-f32e4c4dc1fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33XJaQfE_Kan",
        "outputId": "76de022d-cd36-4c93-f779-be987841852f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Starting Training...\n",
            "Epoch [1/5], Step [100/938], Loss: 0.5497\n",
            "Epoch [1/5], Step [200/938], Loss: 0.1472\n",
            "Epoch [1/5], Step [300/938], Loss: 0.0885\n",
            "Epoch [1/5], Step [400/938], Loss: 0.0561\n",
            "Epoch [1/5], Step [500/938], Loss: 0.0843\n",
            "Epoch [1/5], Step [600/938], Loss: 0.1273\n",
            "Epoch [1/5], Step [700/938], Loss: 0.1546\n",
            "Epoch [1/5], Step [800/938], Loss: 0.1071\n",
            "Epoch [1/5], Step [900/938], Loss: 0.0576\n",
            "Epoch [2/5], Step [100/938], Loss: 0.0234\n",
            "Epoch [2/5], Step [200/938], Loss: 0.0608\n",
            "Epoch [2/5], Step [300/938], Loss: 0.1127\n",
            "Epoch [2/5], Step [400/938], Loss: 0.0129\n",
            "Epoch [2/5], Step [500/938], Loss: 0.0683\n",
            "Epoch [2/5], Step [600/938], Loss: 0.1542\n",
            "Epoch [2/5], Step [700/938], Loss: 0.0635\n",
            "Epoch [2/5], Step [800/938], Loss: 0.1348\n",
            "Epoch [2/5], Step [900/938], Loss: 0.1242\n",
            "Epoch [3/5], Step [100/938], Loss: 0.0255\n",
            "Epoch [3/5], Step [200/938], Loss: 0.1226\n",
            "Epoch [3/5], Step [300/938], Loss: 0.0272\n",
            "Epoch [3/5], Step [400/938], Loss: 0.0542\n",
            "Epoch [3/5], Step [500/938], Loss: 0.0010\n",
            "Epoch [3/5], Step [600/938], Loss: 0.1177\n",
            "Epoch [3/5], Step [700/938], Loss: 0.0483\n",
            "Epoch [3/5], Step [800/938], Loss: 0.0092\n",
            "Epoch [3/5], Step [900/938], Loss: 0.0379\n",
            "Epoch [4/5], Step [100/938], Loss: 0.1189\n",
            "Epoch [4/5], Step [200/938], Loss: 0.0981\n",
            "Epoch [4/5], Step [300/938], Loss: 0.0324\n",
            "Epoch [4/5], Step [400/938], Loss: 0.0572\n",
            "Epoch [4/5], Step [500/938], Loss: 0.0131\n",
            "Epoch [4/5], Step [600/938], Loss: 0.0711\n",
            "Epoch [4/5], Step [700/938], Loss: 0.1361\n",
            "Epoch [4/5], Step [800/938], Loss: 0.0217\n",
            "Epoch [4/5], Step [900/938], Loss: 0.1368\n",
            "Epoch [5/5], Step [100/938], Loss: 0.0299\n",
            "Epoch [5/5], Step [200/938], Loss: 0.2264\n",
            "Epoch [5/5], Step [300/938], Loss: 0.0414\n",
            "Epoch [5/5], Step [400/938], Loss: 0.0147\n",
            "Epoch [5/5], Step [500/938], Loss: 0.0874\n",
            "Epoch [5/5], Step [600/938], Loss: 0.0144\n",
            "Epoch [5/5], Step [700/938], Loss: 0.1020\n",
            "Epoch [5/5], Step [800/938], Loss: 0.0627\n",
            "Epoch [5/5], Step [900/938], Loss: 0.0649\n",
            "Training finished in 7.19 minutes.\n",
            "Accuracy of the model on the 10,000 test images: 98.70%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchinfo import summary\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST Mean & Std\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(AlexNet,self).__init__()\n",
        "    self.features=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1,out_channels=96,kernel_size=11,stride=4,padding=0),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
        "\n",
        "        nn.Conv2d(96,256,kernel_size=5,stride=1,padding=2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
        "\n",
        "\n",
        "        nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # Layer 4\n",
        "        nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # Layer 5\n",
        "        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "    # Adaptive pooling allows us to handle slight variations if needed,\n",
        "    # ensuring the output is always 6x6 per filter before flattening.\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256 * 6 * 6, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(4096, num_classes),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x=self.features(x)\n",
        "    x=self.avgpool(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.classifier(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = AlexNet(num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "summary(model, input_size=(batch_size, 1, 227, 227))\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "total_step = len(train_loader)\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode (enables Dropout)\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f\"Training finished in {(time.time() - start_time)/60:.2f} minutes.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Testing / Evaluation\n",
        "# ---------------------------------------------------------\n",
        "model.eval() # Set model to evaluation mode (disables Dropout)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the 10,000 test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model, input_size=(batch_size, 1, 227, 227))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLTincxtBGpw",
        "outputId": "5678a580-22fa-4b99-da20-1da453669ca9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AlexNet                                  [64, 10]                  --\n",
              "├─Sequential: 1-1                        [64, 256, 6, 6]           --\n",
              "│    └─Conv2d: 2-1                       [64, 96, 55, 55]          11,712\n",
              "│    └─ReLU: 2-2                         [64, 96, 55, 55]          --\n",
              "│    └─MaxPool2d: 2-3                    [64, 96, 27, 27]          --\n",
              "│    └─Conv2d: 2-4                       [64, 256, 27, 27]         614,656\n",
              "│    └─ReLU: 2-5                         [64, 256, 27, 27]         --\n",
              "│    └─MaxPool2d: 2-6                    [64, 256, 13, 13]         --\n",
              "│    └─Conv2d: 2-7                       [64, 384, 13, 13]         885,120\n",
              "│    └─ReLU: 2-8                         [64, 384, 13, 13]         --\n",
              "│    └─Conv2d: 2-9                       [64, 384, 13, 13]         1,327,488\n",
              "│    └─ReLU: 2-10                        [64, 384, 13, 13]         --\n",
              "│    └─Conv2d: 2-11                      [64, 256, 13, 13]         884,992\n",
              "│    └─ReLU: 2-12                        [64, 256, 13, 13]         --\n",
              "│    └─MaxPool2d: 2-13                   [64, 256, 6, 6]           --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [64, 256, 6, 6]           --\n",
              "├─Sequential: 1-3                        [64, 10]                  --\n",
              "│    └─Dropout: 2-14                     [64, 9216]                --\n",
              "│    └─Linear: 2-15                      [64, 4096]                37,752,832\n",
              "│    └─ReLU: 2-16                        [64, 4096]                --\n",
              "│    └─Dropout: 2-17                     [64, 4096]                --\n",
              "│    └─Linear: 2-18                      [64, 4096]                16,781,312\n",
              "│    └─ReLU: 2-19                        [64, 4096]                --\n",
              "│    └─Linear: 2-20                      [64, 10]                  40,970\n",
              "==========================================================================================\n",
              "Total params: 58,299,082\n",
              "Trainable params: 58,299,082\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 67.94\n",
              "==========================================================================================\n",
              "Input size (MB): 13.19\n",
              "Forward/backward pass size (MB): 337.04\n",
              "Params size (MB): 233.20\n",
              "Estimated Total Size (MB): 583.43\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueOQ9bEZRI5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}